{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the Packge\n",
    "This notebook introduces the basics of using Bandits.jl package.\n",
    "\n",
    "First install `MAB.jl` as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import Pkg\n",
    "# Pkg.add( \"MAB\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "To start using the package,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using MAB\n",
    "using Statistics\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package is divide into 3 sub modules:\n",
    "* Algorithms\n",
    "* Arms\n",
    "* Experiments\n",
    "\n",
    "`Algorithms` includes all the available algorithms. `Arms` includes the available arm model. `Experiments` is supposed to include code for running experiments and is a work under progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a bandit\n",
    "For this demo, first we will create a bandit with 5 Bernoulli arms. It is not necessary to create a bandit with arms specified as below to use algorithms, but these kind of bandits can be used to benchmark new algorithms.\n",
    "A bandit is an array of arms and can be defined as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = [ Arms.Bernoulli(0.30),\n",
    "            Arms.Bernoulli(0.40),\n",
    "            Arms.Bernoulli(0.50),\n",
    "            Arms.Bernoulli(0.60),\n",
    "            Arms.Bernoulli(0.70)  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of arms of the bandit as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_arms = length( bandit )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a 5-arm Bernoulli Bandit.\n",
    "\n",
    "Each arm has 3 functions associated with it:\n",
    "* `pull!()` - To pull an arm. Returns a reward associated with the arm. pull!() also changes the internal state of the arm if it is a Markovian/Non-stationary arms.\n",
    "* `tick!()` - To change the internal state of a Markovian/Non-stationary arm. Not necessarily return something. May return junk depending on the internal state.\n",
    "* `reset!()` - To reset the internal state of a Markovian/Non-stationary arm.\n",
    "\n",
    "To pull an 1st arm,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Arms.pull!( bandit[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames(epsGreedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above line will randomly return 0/1 according to the underlying probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Algorithms\n",
    "Every algorithm has specific initializers which depends on the parameters for the algorithms. Usually the first parameter of the initialization is the number of arms of the bandit. As an example, an instance of $\\epsilon$-Greedy algorithm with $\\epsilon = 0.10$ can be created as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsGreedy( 5, 0.05 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg1 = epsGreedy( no_of_arms, 0.05 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run an experiment of $750$ timesteps and average it over $1000$ runs to get an average behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfRounds    = 1000\n",
    "noOfTimesteps = 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an array to hold the results of each round of play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "observations  = zeros( noOfTimesteps, noOfRounds );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the $\\epsilon$-Greedy algorithm over this bandit as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _round = 1:noOfRounds\n",
    "    reset!( alg1 )\n",
    "    for arm ∈ bandit\n",
    "        Arms.reset!( arm )\n",
    "    end\n",
    "    for _n = 1:noOfTimesteps\n",
    "        armToPull = get_arm_index( alg1 )\n",
    "        reward    = Arms.pull!( bandit[armToPull] )\n",
    "        update_reward!( alg1, reward )\n",
    "        observations[_n,_round] = reward\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code runs the algorithm for 250 time steps and save the obtained rewards into `observations`. Note that we need to `reset!()` the algorithm and arms between each rounds.\n",
    "\n",
    "To see the average behaviour, we can plot the average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgReward = mean( observations, dims = 2 )\n",
    "\n",
    "using PyPlot\n",
    "PyPlot.plot( 1:noOfTimesteps, avgReward, label = Algorithms.info_str(alg1) )\n",
    "PyPlot.legend()\n",
    "PyPlot.grid()\n",
    "PyPlot.ylabel( \"Avg. Reward\" )\n",
    "PyPlot.xlabel( \"Timesteps\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats!! You have successfully completed the basics of Bandits.jl package. You can to the documentation page of the package to explore available algorithms and arm models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Multiple Algorithms\n",
    "You can also compare performance of multiple algorithms easily with the package. First we'll look into the actual code for doing it. Later we will look into the `Experiments` section to automate this function.\n",
    "\n",
    "As above, we'll start with defining a bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit1 = [ Arms.Bernoulli(0.25), Arms.Bernoulli(0.35), Arms.Bernoulli(0.45),\n",
    "            Arms.Bernoulli(0.55), Arms.Bernoulli(0.65), Arms.Bernoulli(0.75) ]\n",
    "\n",
    "no_of_arms = length( bandit1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define an array of algorithms which we want to test along the associated parameters as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_algs = [ epsGreedy( no_of_arms, 0.10),\n",
    "              epsGreedy( no_of_arms, 0.20),\n",
    "              UCB1( no_of_arms ),\n",
    "              TS( no_of_arms )    ];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the experiment as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = figure()\n",
    "\n",
    "for _alg ∈ test_algs\n",
    "    observations = zeros( noOfTimesteps, noOfRounds )\n",
    "    for _r = 1:noOfRounds\n",
    "        Algorithms.reset!( _alg )\n",
    "        for _arm ∈ bandit1\n",
    "            Arms.reset!( _arm )\n",
    "        end\n",
    "        for _n = 1:noOfTimesteps\n",
    "            armToPull = Algorithms.get_arm_index( _alg )\n",
    "            reward    = Arms.pull!( bandit1[armToPull] )\n",
    "            Algorithms.update_reward!( _alg, reward )\n",
    "            \n",
    "            observations[_n,_r] = reward\n",
    "        end\n",
    "    end\n",
    "    avgReward = mean( observations, dims =2 );\n",
    "    PyPlot.plot( 1:noOfTimesteps, avgReward, label = Algorithms.info_str(_alg) )\n",
    "end\n",
    "PyPlot.ylabel( \"Avg. Reward\" )\n",
    "PyPlot.xlabel( \"Timesteps\" )\n",
    "PyPlot.title( \"Comparison Plot (Averaged over $noOfRounds runs)\" )\n",
    "PyPlot.ax = gca()\n",
    "PyPlot.ax[:set_ylim]( [0.00, 1.00] )\n",
    "PyPlot.legend()\n",
    "PyPlot.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You explore the above code by changing the bandit model and the algorithms to compare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
