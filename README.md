# Bandits
This package provide a framework for developing and comparing various Bandit algorithms

## Available Algorithms
1. ϵ-greedy
   1. ϵ-greedy
   2. ϵ_n greedy
2. Upper Confidence Bound Policies
   1. UCB1
   2. UCB-Normal
3. Thompson Sampling
   1. Thompson Sampling
   2. Dynamic Thompson Sampling
   3. Optimistic Thompson Sampling
4. EXP3
   1. EXP3
   2. EXP3.1
5. SoftMax
6. REXP3

## Available Arm Models
1. Bernoulli
2. Beta
3. Normal
4. Sinusoidal (without noise)
5. Pulse (without noise)
6. Square
